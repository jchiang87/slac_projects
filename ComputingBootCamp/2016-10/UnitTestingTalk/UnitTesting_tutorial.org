#+STARTUP: beamer
#+LaTeX_CLASS: beamer
#+LaTeX_CLASS_OPTIONS: [10pt, t]
#+BEAMER_FRAME_LEVEL: 1
#+TITLE: Unit Testing and Continuous Integration
#+AUTHOR: Jim Chiang
#+DATE: 2016-10-06
#+COLUMNS: %45ITEM %10BEAMER_env(Env) %8BEAMER_envargs(Env Args) %4BEAMER_col(Col) %8BEAMER_extra(Extra)
#+PROPERTY: BEAMER_col_ALL 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 :ETC
#+OPTIONS: toc:nil
#+LaTeX_HEADER: \newcommand{\code}[1]{{\tt{#1}}}
#+LaTeX_HEADER: \newcommand{\mybold}[1]{{\textbf{#1}}}
#+LaTeX_HEADER: \hypersetup{colorlinks=true, urlcolor=blue}

* Outline
- Why Test Code?
- What is Unit Testing?
- Writing unit tests and test-driven development
- unittest frameworks (py.test)
- examples.
- What is Continuous Integration?
- Using Travis-CI

* Why Test Code?
- We all test code: Any time code is executed and we do something with
  the output, we are testing it at some level.
- Any code base of significant size contains bugs.
- Any new development can introduce bugs or break things in other parts of
  the code.
- Dedicated tests verify, in a systematic way, that the code behaves as
  expected.
- A comprehensive set of tests ensures that the code continues
  to perform as expected if any new development occurs.


* Why unit testing?
There is an extensive litany of types of software testing: system,
integration, regression, functional, black box, white box, regression,
etc..  These many of these overlap in purpose and are combined in
application.

For this talk, we will concentrate on *unit testing*.  This is the
most basic kind of testing.  It exercises the lowest levels of
functionality: functions and class methods, or even single lines of
code, and therefore affects most directly the process of developing
software as it is occurring.

Fowler on Unit testing:
- Make sure tests are fully automatic and that they check their own results.

  It's tedious to run test for which you have to manually check that
  output is correct, and as a result, you will run the tests less
  often.

- A suite of tests is a powerful bug detector that decapitates the time
  it takes to find bugs.

  In the absence of tests, the assertion is that most programming time
  is spent debugging, and most of the time spent debugging is finding
  the bug.  During development, running a fairly complete set of tests
  continuously as you add code will tell you right away when and
  (usually) where you've introduced a bug.

- Run your tests frequently. Localize tests whenever you compile --
  every test at least every day.

  This ensures that any bugs that are introduced during development
  are found right away.  For running a full suite of tests, a CI
  system is indispensible.

- When you get a bug report, start by writing a unit test that
  exposes the bug.

  This helps you indentify the nature of the bug very precisely since
  you want the code that exercises the bug to be as compact as
  possible.  You will know that your development is done when the
  added test passes.  This is the rationale for Test Driven
  Development.

- It is better to write and run incomplete tests than not to run
  complete tests.

  Some tests are better than none even if they are not complete.
  Tests should be written for code that is likely to be complicated to
  implement, i.e., "risky" code; but more precisely, tests should be
  implemented for any code for which its failure would have the
  greatest cost: even if the code is straight-forward, if its used in
  contexts where its failure would break crucial functionality,
  stringent unit tests should be written and run routinely.

- Think of the boundary conditions under which things might go wrong
  and concentrate your tests there.

  This includes things like entry and exit points of loops where
  off-by-one errors occur, cases where defaults are things like empty
  or non-existent files, etc..

- Don't forget to test that exceptions are raised when things are
  expected to go wrong.

  If the code raises exceptions, then that is part of its interface
  and should be subject to tests, especially if client code is
  expected to handle those exceptions in some particular way.

- Don't let the fear that testing can't catch all bugs stop you from
  writing the tests that will catch most bugs.

  100% testing coverage is a nice goal, but it can be a tall mountain
  to climb.  It's better to concentrate ones efforts on implementing
  good tests that will likely prove most useful in maintaining code
  quality.

* Writing Unit Tests
In order for unit testing to be effective,
- Tests should run quickly and automatically.
  One should feel free to run unit tests many times during a session.
- Tests should be self-checking with
  only failures having any output that needs to be parsed.
- Test code should be standalone and self-contained. Dependencies on
  external data (which may have limited accessibility or which may
  change over time, e.g., the contents of a web page) should be kept
  to a minumum.
- Things to test:
  - Test for success: Given specific inputs, does the function or
    method produce the expected outputs?
  - Test for failure: For bad input, does the function or method fail in
    the expected way? e.g., by raising specific exceptions or returning
    specific error codes.

* Ways to Test
  (see https://developer.lsst.io/coding/unit_test_policy.html)
  - White-box Tests
    "These tests are designed by examining the internal logic of each
    module and defining the input data sets that force the execution
    of different paths through the logic."

  - Black-box Tests
    "These tests are designed by examining the specification of each
    module and defining input data sets that will result in different
    behavior (e.g., outputs). Black-box tests should be designed to
    exercise the software for its whole range of inputs."

  - Performance Tests
    "If the detailed design placed resource constraints on the
    performance of a module, compliance with these constraints should
    be tested."

* Test Driven Development
  - Similarity to debugging:
    - Reproduce and isolate the bug.
    - Work on production code until bug is fixed.
  - For new functionality:
    - Write the test code that calls the function or method and tests
      the output.  Since the production code hasn't been written yet,
      this test code will fail.  Production code should not be touched
      (except for refactoring) unless it is to fix a failing test.

    - Add production code until and only until the tests pass ("the
      simplest thing that can possibly work").  This helps prevent
      adding functionality that would not be tested by the test code
      that was just written.

    - Refactor to remove duplicated functionality or to handle special
      cases more generally (being careful not to introduce too much
      new functionality, if any).

    - Redesign interfaces:
      - Update tests.
      - Update production code.

* Unit test examples:
  <

* Using Travis-CI
  - Free for public GitHub-hosted repositories
    - Connect GitHub repo to Travis-CI.

      GitHub repo (as admin):

      Settings -> Webhooks & services -> Add service

      At Travis-CI:

      My Repositories +(Add New Repository) -> <Activate switch>
    - Add a .travis.yml file. (See [[https://github.com/DarkEnergyScienceCollaboration/desc_package_template][desc\_package\_template]] package.)
      - install code and dependencies
      - set up environment
      - run tests and coverage analysis
    - Connect to Coveralls
    - Add badges to GitHub repo.

* Example .travis.yml
\smaller
#+BEGIN_SRC yml
language: C

install:
  - travis_wait ./setup/travis_install.sh lsst-sims nose pandas pylint
  - export PATH="$HOME/miniconda/bin:$PATH"
  - source eups-setups.sh
  - pip install coveralls
  - setup lsst_sims
  - eups declare -r . twinkles -t current
  - setup twinkles

cache:
  directories:
  - $HOME/miniconda.tarball
  timeout: 360
#+END_SRC yml

* Example .travis.yml (continued)
\smaller
#+BEGIN_SRC yml
services:
  - mysql

before_script:
  - mysql -e 'create database myapp_test'
  - mysql -e 'show databases;'

script:
  - nosetests -s --with-coverage --cover-package=desc.twinkles
  - pylint --py3k `find . -name \*.py -print | grep -v workflows`

after_success:
  - coveralls
#+END_SRC yml

* SLCosmo: Description and Use Cases
  SLCosmo is a package to infer cosmological parameters from
  Strong Lensing time delay measurements.
  - Two classes (so far):
    - \code{TDC2ensemble}: container for posterior samples of SL time delays.
    - \code{SLCosmo}: container for \code{TDC2ensemble} objects
      - Creates mocks
      - Reads in persisted files
      - computes cosmological parameter posteriors (sampling priors and
        combining with TD posteriors

  - Use cases:
    - I/O
      - Test output formatting.
      - Test that input files can be read in correctly.
      - Test that I/O consistency.
    - Mock Generation
      - Test that generated mocks have expected properties.
    - Computing joint likelihoods for a collection of lens systems.
      - Test for expected results using standard input data.

* Worked example:
  - [[https://github.com/DarkEnergyScienceCollaboration/SLCosmo/issues/9][GitHub]] issue.
  - [[https://github.com/DarkEnergyScienceCollaboration/SLCosmo/pull/8][Pull request]] for adding the new functionality.
  - The [[https://github.com/DarkEnergyScienceCollaboration/SLCosmo/blob/b84305c37774db27066b28865dca574e5b0b8418/tests/test_TDC2ensemble.py][new test code]].
  - The [[https://travis-ci.org/DarkEnergyScienceCollaboration/SLCosmo/builds/145666391][failure]] of the PR in Travis-CI.

* Pair Programming on Unit Tests
  - Pair up, matching a more experienced developer with a less experienced
    one if possible.
  - Pick one or two unit tests from the [[https://github.com/DarkEnergyScienceCollaboration/SLCosmo/issues][SLCosmo issues]] to implement.
  - Work on the tests using GitHub flow:
\smaller
Fork the repo at
https://github.com/DarkEnergyScienceCollaboration/SLCosmo
#+BEGIN_SRC sh
$ git clone git@github.com:<github userid>/SLCosmo.git
$ cd SLCosmo
$ source setup/setup.sh
$ git checkout -b <descriptive branch name>
$ cd tests
<... add new tests or modify existing tests ...>
$ python test_[SLCosmo,TDC2ensemble].py
<... show that it fails ...>
$ git add test_[SLCosmo,TDC2ensemble].py
$ git commit -m "unit tests for ..."
$ git push -u origin <descriptive branch name>
#+END_SRC sh
Then at https://github.com/<github userid>/SLCosmo go to your
branch and make the pull-request.

* Group Code Review
  We'll pick one or two pull-requests and go through the new code, adding
  comments on the source code as appropriate.

* Wrap-up

